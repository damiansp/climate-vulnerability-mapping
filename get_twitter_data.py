from datetime import datetime
import json

import pandas as pd
from pprint import pprint
from twython import Twython


KEYS = 'config/keys.json'
CALLBACK_URL = 'https://www.climate-vulnerability-mapping.com'
KEYWORDS = ['flood', 'carbon emission']
TWEET_FIELDS = [
    'coordinates', 'created_at', 'id', 'lang', 'place', 'retweeted', 'text']
NOW = datetime.utcnow()



def main():
    keys = get_keys()
    tweets = get_tweets(keys)
    df = convert_tweets(tweets)
    save(df)    

    
def get_keys():
    with open(KEYS, 'r') as f:
        keys = json.load(f)['twitter']
        APP_KEY = keys['apiKey']
        APP_SECRET = keys['apiKeySecret']
        return APP_KEY, APP_SECRET
    twitter = Twython(*keys)
    auth = twitter.get_authentication_tokens(CALLBACK_URL)        
    OAUTH_TOKEN = auth['oauth_token']
    OAUTH_TOKEN_SECRET = auth['oauth_token_secret']
    return APP_KEY, APP_SECRET, OAUTH_TOKEN, OAUTH_TOKEN_SECRET
    

def get_tweets(keys):
    twitter = Twython(*keys)
    data = []
    for kw in KEYWORDS:
        print(f'Getting data for {kw}...')
        res = twitter.search(q=kw, result_type='recent')
        tweets = res['statuses']
        for tweet in tweets:
            keep = {field: tweet[field] for field in TWEET_FIELDS}
            keep['keyword'] = kw
            data.append(keep)
    return data


def convert_tweets(tweets):
    df = pd.DataFrame(data=[tweet.values() for tweet in tweets],
                      columns=TWEET_FIELDS + ['keyword'])
    return df


def save(df):
    hour_str = str(NOW).replace(' ', '-').split(':')[0]
    print(hour_str)
    filename = f'twitter_climate_data_{hour_str}.csv'
    path = f'output/{filename}'
    print(f'Saving data to {path}...')
    df.to_csv(path, index=False)

if __name__ == '__main__':
    main()
